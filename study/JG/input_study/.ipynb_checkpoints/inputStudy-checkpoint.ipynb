{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point\n",
    "What is symptom terminology in the input string?\n",
    "Also, relationship similarity of words is important to seach information\n",
    "\n",
    "\n",
    "### 1. 명확한 symptom으로 이루어진 문장으로 입력받는 경우\n",
    "I have **cough, phlegm, sore throat, rash, and headaches.** Also, \n",
    "\n",
    "### 2. Symptom을 비유하듯이 표현한 경우\n",
    "My **neck and armpit is swollen**, and **red**\n",
    "\n",
    "또는\n",
    "\n",
    "(감기 걸린 것 처럼, 몸이 으실으실하다)\n",
    "My body is **cold shivers** like fever.\n",
    "\n",
    "### 3. 단어(단답)로 입력받는 경우\n",
    "Depression\n",
    "Rapid heart rate\n",
    "Annoyed\n",
    "Anxiety\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = \"I have cough, phlegm, sore throat, rash, blar, today i see the movie and headaches\"\n",
    "input2 = \"My neck and armpit is swollen and red\"\n",
    "input3 = \"Depression | Rapid heart rate | Annoyed | Anxiety\"\n",
    "inputEx = \"SO MUCH PAIN! In the last 2 years I have suffered with a brain tumour so have been in a LOT of pain to the point of morphine everyday for a year. Then I had brain surgery...but the pain from this pill came pretty close!! In serious pain to the point of blacking out hot and cold shivers and just sat in pain feeling like trapped wind/indigestion it&#039;s just.. Ahhhh!!!! Don&#039;t take this!!!\"\n",
    "\n",
    "input1 = input1.lower()\n",
    "input2 = input2.lower()\n",
    "input3 = input3.lower()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization, stemming, lemmatization, and stopword\n",
    "\n",
    "원형으로 바꾼뒤에 어간 추출해야할듯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-134-a1cbfd205538>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0minput1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengStopword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[0minput2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengStopword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0minput3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengStopword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-134-a1cbfd205538>\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(input, engStopword)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengStopword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m->\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mtokenizeWord\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext_to_word_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mtokenizeWord\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mengLemm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizeWord\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokenizeWord\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\stem\\wordnet.py\u001b[0m in \u001b[0;36mlemmatize\u001b[1;34m(self, word, pos)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNOUN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0mlemmas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_morphy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlemmas\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py\u001b[0m in \u001b[0;36m_morphy\u001b[1;34m(self, form, pos, check_exceptions)\u001b[0m\n\u001b[0;32m   1895\u001b[0m         \u001b[1;31m# 0. Check the exception lists\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1896\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcheck_exceptions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1897\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mform\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1898\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfilter_forms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mform\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mform\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1899\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "engLemm = WordNetLemmatizer()\n",
    "engStopword = set(stopwords.words('english'))\n",
    "pst = PorterStemmer()\n",
    "\n",
    "def tokenize(input, engStopword)->list:\n",
    "    tokenizeWord = text_to_word_sequence(input)\n",
    "    tokenizeWord = engLemm.lemmatize(tokenizeWord)\n",
    "    result = []\n",
    "    for word in tokenizeWord:\n",
    "        if word not in engStopword:\n",
    "            result.append(word)\n",
    "    \n",
    "    return result\n",
    "\n",
    "input1 = tokenize(input1, engStopword)\n",
    "input2 = tokenize(input2, engStopword)\n",
    "input3 = tokenize(input3, engStopword)\n",
    "\n",
    "stemmedInput1 = [pst.stem(token) for token in input1]\n",
    "stemmedInput2 = [pst.stem(token) for token in input2]\n",
    "stemmedInput3 = [pst.stem(token) for token in input3]\n",
    "# nltk의 stem을 쓴 이유가 기억 나질 않아 (찾아야 함)\n",
    "        \n",
    "print(stemmedInput1)\n",
    "print(stemmedInput2)\n",
    "print(stemmedInput3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-gram으로 변환\n",
    "sore throat와 같이 symptom이 2개의 corpus로 이루어질 수 있을것 같음\n",
    "###### (?)빨갛게 부어오른 피부 는 세개지 않나?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cough phlegm', 'phlegm sore', 'sore throat', 'throat rash', 'rash blar', 'blar today', 'today see', 'see movi', 'movi headach'] \n",
      " ['neck armpit', 'armpit swollen', 'swollen red'] \n",
      " ['depress rapid', 'rapid heart', 'heart rate', 'rate annoy', 'annoy anxieti']\n"
     ]
    }
   ],
   "source": [
    "biInput1 = list(zip(stemmedInput1, stemmedInput1[1:]))\n",
    "biInput2 = list(zip(stemmedInput2, stemmedInput2[1:]))\n",
    "biInput3 = list(zip(stemmedInput3, stemmedInput3[1:]))\n",
    "\n",
    "def change(tupleData)->list:\n",
    "    result = []\n",
    "    for listData in tupleData:\n",
    "        tmp = list(listData)\n",
    "        result.append(\" \".join(listData))\n",
    "    return result\n",
    "    \n",
    "biInput1 = change(biInput1)\n",
    "biInput2 = change(biInput2)\n",
    "biInput3 = change(biInput3)\n",
    "\n",
    "print(biInput1,\"\\n\",biInput2,\"\\n\",biInput3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge Base 긁어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                pain   chest\n",
      "1       shortness   of breath\n",
      "2                   dizziness\n",
      "3                    asthenia\n",
      "4                        fall\n",
      "                ...          \n",
      "1861      bedridden^bedridden\n",
      "1862               prostatism\n",
      "1863          systolic murmur\n",
      "1864                    frail\n",
      "1865                    fever\n",
      "Name: Symptom, Length: 1866, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#pip install --upgrade beautifulsoup4==4.4.1 --user\n",
    "#pip install --upgrade html5lib==1.0b8 --user\n",
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from html_table_parser import parser_functions as parser\n",
    "\n",
    "url = \"https://people.dbmi.columbia.edu/~friedma/Projects/DiseaseSymptomKB/index.html\"\n",
    "\n",
    "result = urlopen(url)\n",
    "html = result.read()\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "temp = soup.find_all('table')\n",
    "p = parser.make2d(temp[0])\n",
    "\n",
    "df = pd.DataFrame(p[1:],columns = p[0])\n",
    "\n",
    "df['Symptom'] = df['Symptom'].str.replace('\\d+', '')\n",
    "df['Symptom'] = df['Symptom'].str.replace('\\s', ' ')\n",
    "df['Symptom'] = df['Symptom'].str.replace('UMLS', '')\n",
    "df['Symptom'] = df['Symptom'].str.replace(':C_', '')\n",
    "\n",
    "print(df['Symptom'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 해당 단어가 knowledge base에 있는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cough', 'sore', 'throat', 'headach']\n",
      "['neck', 'red']\n",
      "['depress', 'rapid', 'heart', 'rate']\n"
     ]
    }
   ],
   "source": [
    "def oneGramCheck(inputStr)->list:\n",
    "    result = []\n",
    "    for word in inputStr:\n",
    "        for baseWord in df['Symptom']:\n",
    "            if word in baseWord:\n",
    "                result.append(word)\n",
    "                break\n",
    "    return result\n",
    "\n",
    "result1 = oneGramCheck(stemmedInput1)\n",
    "result2 = oneGramCheck(stemmedInput2)\n",
    "result3 = oneGramCheck(stemmedInput3)\n",
    "print(result1)\n",
    "print(result2)\n",
    "print(result3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-gram에서도 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "biResult1 = oneGramCheck(biInput1)\n",
    "print(biResult1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ah\n",
    "\n",
    "1. one-gram에서는 swollen은 knowledge base에서 swelling, or swell로 표현됨. 따라서 원형비교나 다른 비교방법이 필요할듯\n",
    "2. bi-gram의 경우에서도 corpust 구성 순서가 바뀌어 있을 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5454545454545454\n",
      "0.6666666666666666\n",
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "str1 = 'sore_throat'\n",
    "str2 = 'throat sore'\n",
    "\n",
    "ratio = SequenceMatcher(None, str1, str2).ratio()\n",
    "print(ratio)\n",
    "\n",
    "str1 = 'swell'\n",
    "str2 = 'swollen'\n",
    "\n",
    "ratio = SequenceMatcher(None, str1, str2).ratio()\n",
    "print(ratio)\n",
    "\n",
    "str1 = 'swollen'\n",
    "str2 = 'swelling'\n",
    "\n",
    "ratio = SequenceMatcher(None, str1, str2).ratio()\n",
    "print(ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.5이상만 받으면 될듯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cough phlegm\n",
      "phlegm sore\n",
      "sore throat\n",
      "throat rash\n",
      "rash blar\n",
      "blar today\n",
      "today see\n",
      "see movi\n",
      "movi headach\n",
      "neck armpit\n",
      "armpit swollen\n",
      "swollen red\n",
      "depress rapid\n",
      "rapid heart\n",
      "heart rate\n",
      "rate annoy\n",
      "annoy anxieti\n",
      "['sore throat', 'throat rash', 'movi headach']\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "def occureCheck(inputStr)->list:\n",
    "    result = []\n",
    "    for word in inputStr:\n",
    "        print(word)\n",
    "        for baseWord in df['Symptom']:\n",
    "            ratio = SequenceMatcher(None, baseWord, word).ratio()\n",
    "            if ratio > 0.66:\n",
    "                result.append(word)\n",
    "                break\n",
    "    return result\n",
    "\n",
    "result1 = occureCheck(biInput1)\n",
    "result2 = occureCheck(biInput2)\n",
    "result3 = occureCheck(biInput3)\n",
    "print(result1)\n",
    "print(result2)\n",
    "print(result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                pain   chest\n",
      "1       shortness   of breath\n",
      "2                   dizziness\n",
      "3                    asthenia\n",
      "4                        fall\n",
      "                ...          \n",
      "1861      bedridden^bedridden\n",
      "1862               prostatism\n",
      "1863          systolic murmur\n",
      "1864                    frail\n",
      "1865                    fever\n",
      "Name: Symptom, Length: 1866, dtype: object\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
