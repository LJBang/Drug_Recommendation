{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SNMHZ/Drug_Recommendation/blob/master/model/MODEL2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gq-c6jcIeXW3"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.metrics.pairwise import  cosine_similarity\n",
        "\n",
        "df_train = pd.read_csv(\"https://raw.githubusercontent.com/SNMHZ/Drug_Recommendation/master/dataset/lem_train.csv\", parse_dates=[\"date\"], infer_datetime_format=True)\n",
        "df_test = pd.read_csv(\"https://raw.githubusercontent.com/SNMHZ/Drug_Recommendation/master/dataset/lem_test.csv\", parse_dates=[\"date\"], infer_datetime_format=True)\n",
        "df_train = pd.concat([df_train,df_test])\n",
        "\n",
        "print(type(df_train['review']))\n",
        "\n",
        "print(df_train.isnull().values.any())\n",
        "print(df_test.isnull().values.any())\n",
        "\n",
        "print(len(df_train))\n",
        "print(len(df_test))\n",
        "\n",
        "df_train = df_train.dropna(how='any')\n",
        "df_test = df_test.dropna(how='any')\n",
        "\n",
        "print(df_train.isnull().values.any())\n",
        "print(df_test.isnull().values.any())\n",
        "\n",
        "print(len(df_train))\n",
        "print(len(df_test))\n",
        "print(df_train['review'][0])\n",
        "\n",
        "df_train['condition2']=df_train['condition'].apply(lambda x: re.sub('[^a-zA-Z]',' ',x))\n",
        "df_test['condition2']=df_test['condition'].apply(lambda x: re.sub('[^a-zA-Z]',' ',x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bygLY3htGnnf"
      },
      "source": [
        "concon=pd.read_csv('https://raw.githubusercontent.com/SNMHZ/Drug_Recommendation/master/dataset/spaced_conidtion.csv', index_col=0)\n",
        "ori_con=concon['original'].unique()\n",
        "mod_con=concon['modified'].unique()\n",
        "\n",
        "for i in range(len(ori_con)):\n",
        "    ori_con[i]=re.sub('[^a-zA-Z]',' ',ori_con[i])\n",
        "\n",
        "for i in range(len(mod_con)):\n",
        "    mod_con[i]=re.sub('[^a-zA-Z]',' ',mod_con[i])\n",
        "\n",
        "con_dict={}\n",
        "for i in range(len(mod_con)):\n",
        "    con_dict[ori_con[i]]=mod_con[i]\n",
        "\n",
        "print(con_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAZRtjICMIiI"
      },
      "source": [
        "con_re = re.compile('(%s)' % '|'.join(con_dict.keys()))\n",
        "\n",
        "def expand_con(s, con_dict=con_dict):\n",
        "    def replace(match):\n",
        "        return con_dict[match.group(0)]\n",
        "    return con_re.sub(replace, s)\n",
        "\n",
        "df_train['new'] = df_train['review'].str.cat(df_train['condition2'], sep=' ')\n",
        "df_train['new'] = df_train['new'].str.lower()\n",
        "\n",
        "df_train['new100'] = df_train['new'].apply(lambda x: expand_con(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccDwZjiXRNVK"
      },
      "source": [
        "tokenize_data2 = [word_tokenize(sentence) for sentence in df_train['new100']]\n",
        "\n",
        "print(tokenize_data2[:2])\n",
        "\n",
        "model2 = Word2Vec(sentences= tokenize_data2, size=100, window=5, min_count=5, workers=4, sg=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApzUl0uAWtFD"
      },
      "source": [
        "df_train['condition3'] = df_train['condition2'].str.lower().apply(lambda x: expand_con(x))\n",
        "df_test['condition3'] = df_test['condition2'].str.lower().apply(lambda x: expand_con(x))\n",
        "cond_norm=df_train['condition3'].value_counts(True)\n",
        "\n",
        "cond_norm['birthcontrol']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPntvwYaq7yi"
      },
      "source": [
        "def predictCondition(data):\n",
        "    result = {}\n",
        "    # print(words)\n",
        "\n",
        "    for word in data:\n",
        "        try:\n",
        "            tmp = model2.wv.most_similar(word, topn=100)\n",
        "            print(word)\n",
        "            # print(len(tmp))\n",
        "            # print(tmp)\n",
        "            result[word]= tmp\n",
        "        except:\n",
        "            print(word + \" not in here\\n\")\n",
        "    \n",
        "    return result\n",
        "\n",
        "def predictCondition2dict(_predictCondition):\n",
        "  predict_dict = {}\n",
        "  for key, value in _predictCondition:\n",
        "    predict_dict[key] = value\n",
        "  return predict_dict\n",
        "\n",
        "def predictCondition_concat(_dict_list):\n",
        "  sum_dict = {}\n",
        "  for _dict in _dict_list:\n",
        "    for key in _dict.keys():\n",
        "      if key in mod_con:\n",
        "        if key in sum_dict:\n",
        "          #print(_dict[key])\n",
        "          sum_dict[key]+=(_dict[key]*cond_norm[key])\n",
        "        else:\n",
        "          #print(key, _dict[key])\n",
        "          sum_dict[key]=(_dict[key]*cond_norm[key])\n",
        "          #print(sum_dict[key])\n",
        "  return sum_dict\n",
        "\n",
        "def predictConditionSum(data):\n",
        "  predict_res = predictCondition(data)\n",
        "  m_list = []\n",
        "  for key in predict_res:\n",
        "    m_list.append(predictCondition2dict(predict_res[key]))\n",
        "  return predictCondition_concat(m_list)\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "n=WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def lemlem(msg):\n",
        "  msg=msg.replace(\"&#039;\", \"\")\n",
        "  msg=msg.replace(r'[^\\w\\d\\s]',' ')\n",
        "  msg=re.sub('[^a-zA-Z]',' ',msg)\n",
        "  low_msg = msg.lower().split()\n",
        "  stop_msg=[]\n",
        "  for w in low_msg: \n",
        "    if w not in stop_words: \n",
        "        stop_msg.append(w) \n",
        "  result=[n.lemmatize(w) for w in stop_msg]\n",
        "  return result\n",
        "\n",
        "def isincheck(_str):\n",
        "  if len(set(only_condition.isin([_str.lower()])))==2:\n",
        "    return True\n",
        "  return False\n",
        "\n",
        "def findDrugName(condition):\n",
        "\n",
        "    tempIndex = df_train['condition3'] == condition.lower()\n",
        "    # 해당 컨디션 단어와 완전 일치하는 경우 약이 매칭 되도록 함\n",
        "    #tempIndex = df_all['condition_lower'].str.contains(condition.lower())\n",
        "    # 해당 컨디션 단어가 들어만 가도 약이 매칭 되도록 함\n",
        "    #print(\"data type is = \", type(df_all))\n",
        "    \n",
        "    tempSet = df_train[['condition', 'drugName', 'rating']][tempIndex]\n",
        "\n",
        "    result = tempSet.sort_values(by=['rating'], axis=0, ascending=False)\n",
        "    result=result.groupby(['drugName'], as_index=False).mean()\n",
        "    result=result.sort_values(by=['rating'], axis=0, ascending=False)\n",
        "\n",
        "    print(\"condition is \", condition, '\\n',result['drugName'][:5],\"\\n=====================\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EM6wXZ0zpe2X"
      },
      "source": [
        "while True:\n",
        "    msg = input(\"data : \")\n",
        "\n",
        "    tmp = predictConditionSum(lemlem(msg))\n",
        "\n",
        "    m_sorted_res = sorted(list(tmp.items()), key=lambda x:x[1], reverse=True)[:1000]\n",
        "\n",
        "    for j in m_sorted_res[:10]:\n",
        "      findDrugName(j[0])\n",
        "\n",
        "    #only_condition = pd.concat([df_train['condition3'], df_test['condition3']], axis=0)\n",
        "    #only_condition = only_condition.drop_duplicates()\n",
        "    #for i in only_condition.index:\n",
        "    #    only_condition[i] = only_condition[i].lower()\n",
        "    #for i, res in enumerate(m_sorted_res):\n",
        "    #    if isincheck(res[0]):\n",
        "    #        print(i+1, res)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}